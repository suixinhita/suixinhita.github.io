<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta name="description" content="用于写一些学习心得的自用blog" />
      
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> RHODES ISLAND</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/ark1.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/cover_0.png" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">RHODES ISLAND</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.staticfile.org/typed.js/2.0.12/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['欢迎来到罗德岛', 'Arknights', '博士，您还有许多事情需要处理。现在还不能休息哦。'],
        startDelay: 0,
        typeSpeed: 200,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  
  
<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content">一个刀客塔的自留地。</div>
</div>


<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-RL学习-2"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2024/04/27/RL%E5%AD%A6%E4%B9%A0-2/"
    >RL学习-2</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2024/04/27/RL%E5%AD%A6%E4%B9%A0-2/" class="article-date">
  <time datetime="2024-04-27T07:47:59.000Z" itemprop="datePublished">2024-04-27</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
   
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-Python基础学习"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2024/04/27/Python%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"
    >Python基础学习</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2024/04/27/Python%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2024-04-27T07:38:12.000Z" itemprop="datePublished">2024-04-27</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1><span id="python-基础学习">Python 基础学习</span></h1><p>这里的链接均无授权，如有问题邮件告知我会删除（。</p>
<h2><span id="1-数据结构">1 数据结构</span></h2><h3><span id="11-队列">1.1 队列</span></h3><h4><span id="111-双向队列">1.1.1 双向队列</span></h4><p>使用collection模块下的deque:<a target="_blank" rel="noopener" href="https://blog.csdn.net/chl183/article/details/106958004">教程指路：CSDN</a></p>
<h2><span id="5-常用函数">5 常用函数</span></h2><ol>
<li>zip: 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。<a target="_blank" rel="noopener" href="https://www.runoob.com/python/python-func-zip.html">教程指路：基础教程</a></li>
<li>random库，可以用于单次或重复随机抽样，也可以用于生成随机数等。<a target="_blank" rel="noopener" href="https://www.runoob.com/python3/python-random.html">教程指路：基础教程</a></li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-RL学习-1"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2024/04/22/RL%E5%AD%A6%E4%B9%A0-1/"
    >RL学习-1</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2024/04/22/RL%E5%AD%A6%E4%B9%A0-1/" class="article-date">
  <time datetime="2024-04-22T06:28:43.902Z" itemprop="datePublished">2024-04-22</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/RL/">RL</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1><span id="rl学习-基础">RL学习-基础</span></h1><blockquote>
<p>学习参考：<a target="_blank" rel="noopener" href="https://hrl.boyuai.com/">动手学强化学习</a>和<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1sd4y167NS/">强化学习的数学原理</a></p>
</blockquote>
<h2><span id="1-多臂老虎机问题">1 多臂老虎机问题</span></h2><p>问题基于多臂老虎机开展，其各个拉臂之间相互独立，对老虎机进行类封装，方便进行后续探究。</p>
<h3><span id="11-epsilon-greedy方法">1.1 epsilon-greedy方法</span></h3><p>按照书中内容，这里主要展示的是epsilon逐渐下降的收敛优势。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bandit</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,k</span>):</span><br><span class="line">        self.prob=np.random.uniform(size=k)</span><br><span class="line">        self.max_index=np.argmax(self.prob)</span><br><span class="line">        self.max_prob=np.<span class="built_in">max</span>(self.prob)</span><br><span class="line">        self.K=k</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self,now</span>):</span><br><span class="line">        <span class="keyword">if</span> np.random.rand()&lt;self.prob[now]:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solver</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,Bandit</span>):</span><br><span class="line">        self.bandit=Bandit</span><br><span class="line">        self.counts=np.zeros(Bandit.K)</span><br><span class="line">        self.regrets=[]</span><br><span class="line">        self.actions=[]</span><br><span class="line">        self.total_regret=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_regret</span>(<span class="params">self,k</span>):</span><br><span class="line">        self.total_regret+=self.bandit.max_prob-self.bandit.prob[k]</span><br><span class="line">        self.regrets.append(self.total_regret)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run_one_step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;error?&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run_n_times</span>(<span class="params">self,n</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            k=self.run_one_step()</span><br><span class="line">            self.counts[k]+=<span class="number">1</span></span><br><span class="line">            self.actions.append(k)</span><br><span class="line">            self.update_regret(k)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EpsilonGreedy</span>(<span class="title class_ inherited__">Solver</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,bandit,prob_init</span>):</span><br><span class="line">    <span class="comment"># def __init__(self,bandit,epsilon,prob_init):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(bandit)</span><br><span class="line">        <span class="comment"># self.epsilon=epsilon</span></span><br><span class="line">        self.total_count=<span class="number">0</span></span><br><span class="line">        self.epsilon = <span class="number">0</span></span><br><span class="line">        self.estimate_reward=np.full(bandit.K,prob_init)</span><br><span class="line">    <span class="comment">#被注释掉的是不变的epsilon,现在是变epsilon=1/t</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run_one_step</span>(<span class="params">self</span>):</span><br><span class="line">        self.total_count+=<span class="number">1</span></span><br><span class="line">        self.epsilon=<span class="number">1</span>/self.total_count</span><br><span class="line">        tmp=np.random.random()</span><br><span class="line">        <span class="keyword">if</span> tmp&lt;self.epsilon:</span><br><span class="line">            k=np.random.randint(<span class="number">0</span>,self.bandit.K)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            k=np.argmax(self.estimate_reward)</span><br><span class="line">        reward=self.bandit.step(k)</span><br><span class="line">        self.estimate_reward[k]+=<span class="number">1</span>/(<span class="number">1</span>+self.counts[k])*(reward-self.estimate_reward[k])</span><br><span class="line">        <span class="keyword">return</span> k</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_results</span>(<span class="params">solvers, solver_names</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成累积懊悔随时间变化的图像。输入solvers是一个列表,列表中的每个元素是一种特定的策略。</span></span><br><span class="line"><span class="string">    而solver_names也是一个列表,存储每个策略的名称&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> idx, solver <span class="keyword">in</span> <span class="built_in">enumerate</span>(solvers):</span><br><span class="line">        time_list = <span class="built_in">range</span>(<span class="built_in">len</span>(solver.regrets))</span><br><span class="line">        plt.plot(time_list, solver.regrets, label=solver_names[idx])</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Time steps&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Cumulative regrets&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;%d-armed bandit&#x27;</span> % solvers[<span class="number">0</span>].bandit.K)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">bandit_10_arm=Bandit(<span class="number">10</span>)</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">epsilon_greedy_solver = EpsilonGreedy(bandit_10_arm,<span class="number">1.0</span>)</span><br><span class="line">epsilon_greedy_solver.run_n_times(<span class="number">5000</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;epsilon-贪婪算法的累积懊悔为：&#x27;</span>, epsilon_greedy_solver.regrets)</span><br><span class="line">plot_results([epsilon_greedy_solver], [<span class="string">&quot;EpsilonGreedy&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># np.random.seed(1)</span></span><br><span class="line"><span class="comment"># k=10</span></span><br><span class="line"><span class="comment"># test_bandit=Bandit(k)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="12-上置信界算法ucb">1.2 上置信界算法（UCB）</span></h3><p>引入一个不确定度度量$U(a_t)$，这个不确定度会根据该动作尝试次数的增加而降低。</p>
<blockquote>
<p>依据：$X_i$为随机变量，其满足霍夫丁不等式：<br>$P(E(X)&lt;x(a_t)+u)&gt;1-e^{-2nu^2}$</p>
</blockquote>
<p>期望报酬$\hat{Q}(a_t)$为此时的$x(a_t)$，而不确定度度量$\hat{U}(a_t)$则代入其中的$u$，可以得到概率$p&#x3D;e^{-2N(a_t)(\hat{U}(a_t))^2}$，当其很小时候，大概率期望报酬的上界就是$\hat{Q}(a_t)+\hat{U}(a_t)$。此时可以考虑取期望报酬上界最大的动作，而非取期望报酬最大的动作。<br>当设定一个$p$，就可以求得对应的不确定度度量$\hat{U}(a_t)&#x3D;\sqrt{\frac{\log(p)}{-2N(a_t)}}$，进而得到其期望报酬上界。<br><em>更直观地说，UCB 算法在每次选择拉杆前，先估计每根拉杆的期望奖励的上界，使得拉动每根拉杆的期望奖励只有一个较小的概率超过这个上界，接着选出期望奖励上界最大的拉杆，从而选择最有可能获得最大期望奖励的拉杆。（原书语，感觉没直观到哪）</em><br>事实上，在真正应用时候，可以用一个系数$c$把不确定性压下来，如：$\hat{Q}’(a_t)&#x3D;\hat{Q}(a_t)+c\times\hat{U}(a_t)$，事实上这个$c$应该可以表示所谓探索和剥削之间的比率，$c$越大越探索，越小越剥削。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UCB</span>(<span class="title class_ inherited__">Solver</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,bandit,coef,prob_init</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(bandit)</span><br><span class="line">        self.prob=<span class="number">0</span> <span class="comment">#p=1/t</span></span><br><span class="line">        self.total_count=<span class="number">0</span></span><br><span class="line">        self.estimate_reward=np.full(self.bandit.K,prob_init)</span><br><span class="line">        self.coef=coef</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run_one_step</span>(<span class="params">self</span>):</span><br><span class="line">        self.total_count+=<span class="number">1</span></span><br><span class="line">        self.prob=<span class="number">1</span>/self.total_count</span><br><span class="line">        Ucb=self.estimate_reward+self.coef*np.sqrt((np.log(self.prob))/(-<span class="number">2</span>*(self.counts+<span class="number">1</span>)))</span><br><span class="line">        k=np.argmax(Ucb)</span><br><span class="line">        reward = self.bandit.step(k)</span><br><span class="line">        self.estimate_reward[k] += <span class="number">1</span> / (<span class="number">1</span> + self.counts[k]) * (reward - self.estimate_reward[k])</span><br><span class="line">        <span class="keyword">return</span> k</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3><span id="13-汤普森采样方法">1.3 汤普森采样方法</span></h3><p>一种蒙特卡洛（用频率估计概率）的方法，其主要认为根据对某个动作$a_t$的概率可以假设其符合beta分布（一种根据先验知识假设二项分布概率密度函数的分布（<em>个人见解</em>）其参数分别为$(a,b)$，对于拉杆$k$而言，其成功次数$n_k$和失败次数$m_k$构成的Beta分布满足$Beta(n_k+1,m_k+1)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TompsonSampling</span>(<span class="title class_ inherited__">Solver</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,bandit</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(bandit)</span><br><span class="line">        self.a=np.ones(bandit.K)</span><br><span class="line">        self.b=np.ones(bandit.K)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run_one_step</span>(<span class="params">self</span>):</span><br><span class="line">        samples=np.random.beta(self.a,self.b)</span><br><span class="line">        k=np.argmax(samples)</span><br><span class="line">        reward = self.bandit.step(k)</span><br><span class="line">        <span class="keyword">if</span> reward==<span class="number">1</span>:</span><br><span class="line">            self.a[k]+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.b[k]+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> k</span><br></pre></td></tr></table></figure>
<p>对于老虎机问题，还没有引入状态变量$S_t$，其与环境的交互并不会改变环境，即多臂老虎机的每次交互的结果和以往的动作无关。</p>
<h2><span id="2-马尔科夫过程">2 马尔科夫过程</span></h2><p>关于马尔可夫过程中重要的几个概念：</p>
<blockquote>
<p>概率矩阵&#x2F;概率函数表示状态转移的情况：<br>$P&#x3D;\begin{bmatrix}<br> p_{1,1} &amp; … &amp; p_{1,n}\<br> : &amp;  &amp; :\<br> p_{n,1} &amp; … &amp; p_{n,n}<br>\end{bmatrix}$<br>回报：<br>$G_t&#x3D;R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+…&#x3D;\sum_{i&#x3D;0}^{N}\gamma^{i} R_{t+i} $<br>价值函数（state value）：<br>$V(s)&#x3D;\mathbb{E} [R_t+\gamma G_{t+1}\mid S_{t}&#x3D;s]&#x3D;\mathbb{E} [R_t+\gamma V(S_{t+1})\mid S_{t}&#x3D;s]$<br>价值函数（action value）：<br>$Q^{\pi }(s,a)&#x3D;r(s,a)+\gamma \sum _{s’\in S}P^{\pi}(s’|s,a)V^{\pi}(s’)$</p>
</blockquote>
<h3><span id="21-一个例子">2.1 一个例子</span></h3><p><img src="/.com//example-1.png"><br>可以简单地给出一个路线，求得其回报，也可以求其价值函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">P=[</span><br><span class="line">    [<span class="number">0.9</span>,<span class="number">0.1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.5</span>,<span class="number">0</span>,<span class="number">0.5</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0.6</span>,<span class="number">0</span>,<span class="number">0.4</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0.3</span>,<span class="number">0.7</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.5</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">]</span><br><span class="line">P=np.array(P)</span><br><span class="line">rewards=[-<span class="number">1</span>,-<span class="number">2</span>,-<span class="number">2</span>,<span class="number">10</span>,<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_return</span>(<span class="params">chain,gammar</span>):</span><br><span class="line">    total_reward=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(chain)-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">        <span class="built_in">print</span>(i,chain[i])</span><br><span class="line">        total_reward=gammar*total_reward+rewards[chain[i]-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> total_reward</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_state_value_simple</span>(<span class="params">gammar</span>):</span><br><span class="line">    I=np.eye(<span class="number">6</span>,<span class="number">6</span>)</span><br><span class="line">    reward_=(np.array(rewards)).reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(I)</span><br><span class="line">    tmp=np.linalg.inv((I-gammar*P))</span><br><span class="line">    <span class="built_in">print</span>(tmp)</span><br><span class="line">    value_s=np.dot(tmp,reward_)</span><br><span class="line">    <span class="built_in">print</span>(np.shape(value_s))</span><br><span class="line">    <span class="keyword">return</span> value_s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Chain=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">6</span>]</span><br><span class="line"><span class="comment">#print(cal_return(Chain,0.5))</span></span><br><span class="line"><span class="built_in">print</span>(cal_state_value_simple(<span class="number">0.5</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>此外，对于这部分的一些基础知识推导，参照《强化学习的数学原理》即可。</p>
<h2><span id="3-dp">3 DP</span></h2><p>该部分可能与算法中的DP有所不同，但总之思想应该相似。</p>
<h3><span id="31-值迭代和策略迭代">3.1 值迭代和策略迭代</span></h3><p>网格地图，有终点，禁区和普通区域，需要最短化路径同时尽量不能进入禁区。</p>
<p>此处给出了值迭代和策略迭代两种DP方法的代码，其算法思想和伪代码见笔记。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CliffWalkingEnv</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 悬崖漫步环境&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ncol=<span class="number">12</span>, nrow=<span class="number">4</span></span>):</span><br><span class="line">        self.ncol = ncol  <span class="comment"># 定义网格世界的列</span></span><br><span class="line">        self.nrow = nrow  <span class="comment"># 定义网格世界的行</span></span><br><span class="line">        <span class="comment"># 转移矩阵P[state][action] = [(p, next_state, reward, done)]包含下一个状态和奖励</span></span><br><span class="line">        self.P = self.createP()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">createP</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        P = [[[] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.nrow * self.ncol)]</span><br><span class="line">        <span class="comment"># 4种动作, change[0]:上,change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)</span></span><br><span class="line">        <span class="comment"># 定义在左上角</span></span><br><span class="line">        change = [[<span class="number">0</span>, -<span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [-<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>]]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.nrow):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(self.ncol):</span><br><span class="line">                <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    <span class="comment"># 位置在悬崖或者目标状态,因为无法继续交互,任何动作奖励都为0</span></span><br><span class="line">                    <span class="keyword">if</span> i == self.nrow - <span class="number">1</span> <span class="keyword">and</span> j &gt; <span class="number">0</span>:</span><br><span class="line">                        P[i * self.ncol + j][a] = [(<span class="number">1</span>, i * self.ncol + j, <span class="number">0</span>, <span class="literal">True</span>)]</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="comment"># 其他位置</span></span><br><span class="line">                    next_x = <span class="built_in">min</span>(self.ncol - <span class="number">1</span>, <span class="built_in">max</span>(<span class="number">0</span>, j + change[a][<span class="number">0</span>]))</span><br><span class="line">                    next_y = <span class="built_in">min</span>(self.nrow - <span class="number">1</span>, <span class="built_in">max</span>(<span class="number">0</span>, i + change[a][<span class="number">1</span>]))</span><br><span class="line">                    <span class="comment">#防止超限罢了</span></span><br><span class="line">                    next_state = next_y * self.ncol + next_x</span><br><span class="line">                    reward = -<span class="number">1</span></span><br><span class="line">                    done = <span class="literal">False</span></span><br><span class="line">                    <span class="comment"># 下一个位置在悬崖或者终点</span></span><br><span class="line">                    <span class="keyword">if</span> next_y == self.nrow - <span class="number">1</span> <span class="keyword">and</span> next_x &gt; <span class="number">0</span>:</span><br><span class="line">                        done = <span class="literal">True</span></span><br><span class="line">                        <span class="keyword">if</span> next_x != self.ncol - <span class="number">1</span>:  <span class="comment"># 下一个位置在悬崖</span></span><br><span class="line">                            reward = -<span class="number">100</span></span><br><span class="line">                    P[i * self.ncol + j][a] = [(<span class="number">1</span>, next_state, reward, done)]</span><br><span class="line">        <span class="keyword">return</span> P</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PolicyIteration</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 策略迭代算法 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, env, theta, gamma</span>):</span><br><span class="line">        self.env = env</span><br><span class="line">        self.v = [<span class="number">0</span>] * self.env.ncol * self.env.nrow  <span class="comment"># 初始化价值为0</span></span><br><span class="line">        self.pi = [[<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>]</span><br><span class="line">                   <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow)]  <span class="comment"># 初始化为均匀随机策略</span></span><br><span class="line">        self.theta = theta  <span class="comment"># 策略评估收敛阈值</span></span><br><span class="line">        self.gamma = gamma  <span class="comment"># 折扣因子</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">policy_evaluation</span>(<span class="params">self</span>):  <span class="comment"># 策略评估</span></span><br><span class="line">        cnt = <span class="number">1</span>  <span class="comment"># 计数器</span></span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            max_diff = <span class="number">0</span></span><br><span class="line">            new_v = [<span class="number">0</span>] * self.env.ncol * self.env.nrow</span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow):</span><br><span class="line">                qsa_list = []  <span class="comment"># 开始计算状态s下的所有Q(s,a)价值</span></span><br><span class="line">                <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    qsa = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:</span><br><span class="line">                        p, next_state, r, done = res</span><br><span class="line">                        qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))</span><br><span class="line">                        <span class="comment"># 本章环境比较特殊,奖励和下一个状态有关,所以需要和状态转移概率相乘</span></span><br><span class="line">                    qsa_list.append(self.pi[s][a] * qsa)</span><br><span class="line">                new_v[s] = <span class="built_in">sum</span>(qsa_list)  <span class="comment"># 状态价值函数和动作价值函数之间的关系</span></span><br><span class="line">                max_diff = <span class="built_in">max</span>(max_diff, <span class="built_in">abs</span>(new_v[s] - self.v[s]))</span><br><span class="line">            self.v = new_v</span><br><span class="line">            <span class="keyword">if</span> max_diff &lt; self.theta: <span class="keyword">break</span>  <span class="comment"># 满足收敛条件,退出评估迭代</span></span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;策略评估进行%d轮后完成&quot;</span> % cnt)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">policy_improvement</span>(<span class="params">self</span>):  <span class="comment"># 策略提升</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.nrow * self.env.ncol):</span><br><span class="line">            qsa_list = []</span><br><span class="line">            <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                qsa = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:</span><br><span class="line">                    p, next_state, r, done = res</span><br><span class="line">                    qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))</span><br><span class="line">                qsa_list.append(qsa)</span><br><span class="line">            maxq = <span class="built_in">max</span>(qsa_list)</span><br><span class="line">            cntq = qsa_list.count(maxq)  <span class="comment"># 计算有几个动作得到了最大的Q值</span></span><br><span class="line">            <span class="comment"># 让这些动作均分概率</span></span><br><span class="line">            self.pi[s] = [<span class="number">1</span> / cntq <span class="keyword">if</span> q == maxq <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> q <span class="keyword">in</span> qsa_list]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;策略提升完成&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.pi</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">policy_iteration</span>(<span class="params">self</span>):  <span class="comment"># 策略迭代</span></span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            self.policy_evaluation()</span><br><span class="line">            old_pi = copy.deepcopy(self.pi)  <span class="comment"># 将列表进行深拷贝,方便接下来进行比较</span></span><br><span class="line">            new_pi = self.policy_improvement()</span><br><span class="line">            <span class="keyword">if</span> old_pi == new_pi: <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_agent</span>(<span class="params">agent, action_meaning, disaster=[], end=[]</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;状态价值：&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(agent.env.nrow):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(agent.env.ncol):</span><br><span class="line">            <span class="comment"># 为了输出美观,保持输出6个字符</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;%6.6s&#x27;</span> % (<span class="string">&#x27;%.3f&#x27;</span> % agent.v[i * agent.env.ncol + j]), end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;策略：&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(agent.env.nrow):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(agent.env.ncol):</span><br><span class="line">            <span class="comment"># 一些特殊的状态,例如悬崖漫步中的悬崖</span></span><br><span class="line">            <span class="keyword">if</span> (i * agent.env.ncol + j) <span class="keyword">in</span> disaster:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;****&#x27;</span>, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">            <span class="keyword">elif</span> (i * agent.env.ncol + j) <span class="keyword">in</span> end:  <span class="comment"># 目标状态</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;EEEE&#x27;</span>, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                a = agent.pi[i * agent.env.ncol + j]</span><br><span class="line">                pi_str = <span class="string">&#x27;&#x27;</span></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(action_meaning)):</span><br><span class="line">                    pi_str += action_meaning[k] <span class="keyword">if</span> a[k] &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;o&#x27;</span></span><br><span class="line">                <span class="built_in">print</span>(pi_str, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ValueIteration</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,env,theta,gamma</span>):</span><br><span class="line">        self.env=env</span><br><span class="line">        self.v=[<span class="number">0</span>]*self.env.ncol*self.env.nrow</span><br><span class="line">        self.theta=theta</span><br><span class="line">        self.gamma=gamma</span><br><span class="line">        self.pi=[<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol*self.env.nrow)] <span class="comment">#最终policy</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">value_iteration</span>(<span class="params">self</span>):</span><br><span class="line">        cnt=<span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            max_diff=<span class="number">0</span></span><br><span class="line">            new_v=[<span class="number">0</span>]*self.env.ncol*self.env.nrow</span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol*self.env.nrow):</span><br><span class="line">                qsa_list=[]</span><br><span class="line">                <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    qsa=<span class="number">0</span></span><br><span class="line">                    <span class="keyword">for</span> pair_now <span class="keyword">in</span> self.env.P[s][a]:</span><br><span class="line">                        p,next_s,r,done=pair_now</span><br><span class="line">                        <span class="keyword">if</span> done==<span class="literal">False</span>:</span><br><span class="line">                            qsa += p * (r + self.gamma * self.v[next_s])</span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            qsa += p * r</span><br><span class="line">                    qsa_list.append(qsa)</span><br><span class="line">                    <span class="built_in">print</span>(s,qsa_list)</span><br><span class="line">                new_v[s]=<span class="built_in">max</span>(qsa_list)</span><br><span class="line">                max_diff=<span class="built_in">max</span>(max_diff,<span class="built_in">abs</span>(self.v[s]-new_v[s]))</span><br><span class="line">            self.v=new_v</span><br><span class="line">            <span class="keyword">if</span> max_diff&lt;=theta:<span class="keyword">break</span></span><br><span class="line">            cnt+=<span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;值评估进行%d轮后完成&quot;</span> % cnt)</span><br><span class="line">        self.get_best_policy()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_best_policy</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow):</span><br><span class="line">            qsa_list = []</span><br><span class="line">            <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                qsa = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> pair_now <span class="keyword">in</span> self.env.P[s][a]:</span><br><span class="line">                    p, next_s, r, done = pair_now</span><br><span class="line">                    <span class="keyword">if</span> done == <span class="literal">False</span>:</span><br><span class="line">                        qsa += p * (r + self.gamma * self.v[next_s])</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        qsa += p * r</span><br><span class="line">                qsa_list.append(qsa)</span><br><span class="line">            qsa_max= <span class="built_in">max</span>(qsa_list)</span><br><span class="line">            cnt_max=qsa_list.count(qsa_max)</span><br><span class="line">            self.pi[s] = [<span class="number">1</span> / cnt_max <span class="keyword">if</span> q == qsa_max <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> q <span class="keyword">in</span> qsa_list]</span><br><span class="line">            <span class="comment"># print(s,qsa_list)</span></span><br><span class="line"></span><br><span class="line">env = CliffWalkingEnv()</span><br><span class="line">action_meaning = [<span class="string">&#x27;^&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;&lt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>]</span><br><span class="line">theta = <span class="number">0.001</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line"><span class="comment"># agent = PolicyIteration(env, theta, gamma)</span></span><br><span class="line"><span class="comment"># agent.policy_iteration()</span></span><br><span class="line">agent = ValueIteration(env, theta, gamma)</span><br><span class="line">agent.value_iteration()</span><br><span class="line">print_agent(agent, action_meaning, <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">37</span>, <span class="number">47</span>)), [<span class="number">47</span>])</span><br></pre></td></tr></table></figure>

<h3><span id="32-冰湖环境">*3.2 冰湖环境</span></h3><p>冰湖是 OpenAI Gym 库中的一个环境。OpenAI Gym 库中包含了很多有名的环境，例如 Atari 和 MuJoCo，并且支持定制自己的环境。<br>冰湖环境和悬崖漫步环境相似，也是一个网格世界，大小为4*4。每一个方格是一个状态$S$，智能体起点状态在左上角，目标状态$G$在右下角，中间还有若干冰洞$H$。在每一个状态都可以采取上、下、左、右 4 个动作。由于智能体在冰面行走，因此每次行走都有一定的概率滑行到附近的其它状态，并且到达冰洞或目标状态时行走会提前结束。每一步行走的奖励是 0，到达目标的奖励是 1。<br>此时</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">env = gym.make(<span class="string">&quot;FrozenLake-v1&quot;</span>,render_mode=<span class="string">&quot;human&quot;</span>)  <span class="comment"># 创建环境</span></span><br><span class="line">env = env.unwrapped  <span class="comment"># 解封装才能访问状态转移矩阵P</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PolicyIteration</span>:</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ValueIteration</span>:</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_agent</span>(<span class="params">agent, action_meaning, disaster=[], end=[]</span>):</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个动作意义是Gym库针对冰湖环境事先规定好的</span></span><br><span class="line">action_meaning = [<span class="string">&#x27;&lt;&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;^&#x27;</span>]</span><br><span class="line">theta = <span class="number">1e-5</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">agent = ValueIteration(env, theta, gamma)</span><br><span class="line">agent.value_iteration()</span><br><span class="line"><span class="comment"># agent = PolicyIteration(env, theta, gamma)</span></span><br><span class="line"><span class="comment"># agent.policy_iteration()</span></span><br><span class="line">print_agent(agent, action_meaning, [<span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">12</span>], [<span class="number">15</span>])</span><br></pre></td></tr></table></figure>

<h2><span id="4-td时序差分">4 TD（时序差分）</span></h2><p>TD的目的是求解一个无模型的贝尔曼公式。</p>
<h3><span id="41-sarsa">4.1 Sarsa</span></h3><p>Sarsa算法，用于估计action value，一般用epsilon-greedy来搜索。伪代码和算法推导见笔记。</p>
<p>基于悬崖漫步的Sarsa代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CliffWalking</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, col, row</span>):</span><br><span class="line">        self.col = col</span><br><span class="line">        self.row = row</span><br><span class="line">        self.x = <span class="number">0</span></span><br><span class="line">        self.y = row - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_next_step</span>(<span class="params">self, action</span>):</span><br><span class="line">        actions = [[<span class="number">0</span>, -<span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [-<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>]]</span><br><span class="line">        self.x = <span class="built_in">min</span>(<span class="built_in">max</span>(self.x + actions[action][<span class="number">0</span>], <span class="number">0</span>), self.col - <span class="number">1</span>)</span><br><span class="line">        self.y = <span class="built_in">min</span>(<span class="built_in">max</span>(self.y + actions[action][<span class="number">1</span>], <span class="number">0</span>), self.row - <span class="number">1</span>)</span><br><span class="line">        next_state = self.y * self.col + self.x</span><br><span class="line">        reward = -<span class="number">1</span></span><br><span class="line">        done = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> (self.y == self.row - <span class="number">1</span>) <span class="keyword">and</span> self.x &gt; <span class="number">0</span>:</span><br><span class="line">            done = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> self.x != self.col - <span class="number">1</span>:</span><br><span class="line">                reward = -<span class="number">100</span></span><br><span class="line">        <span class="keyword">return</span> next_state, reward, done</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.x = <span class="number">0</span></span><br><span class="line">        self.y = self.row - <span class="number">1</span></span><br><span class="line">        next_state = self.y * self.col + self.x</span><br><span class="line">        <span class="keyword">return</span> next_state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Sarsa</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, env, gamma, alpha, epsilon, n_action</span>):</span><br><span class="line">        self.env = env</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.n_action = n_action</span><br><span class="line">        self.Q_table = np.zeros([self.env.col * self.env.row, n_action])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">take_action</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="keyword">if</span> (np.random.random() &lt; self.epsilon):</span><br><span class="line">            action = np.random.randint(self.n_action)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.argmax(self.Q_table[state])</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_q_value</span>(<span class="params">self, at, st, r, at1, st1</span>):</span><br><span class="line">        TD_error =  (r + self.gamma * self.Q_table[st1,at1])-self.Q_table[st,at]</span><br><span class="line">        self.Q_table[st,at] += self.alpha * TD_error</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_best_policy</span>(<span class="params">self, state</span>):</span><br><span class="line">        Q_max = np.<span class="built_in">max</span>(self.Q_table[state])</span><br><span class="line">        a = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_action)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_action):</span><br><span class="line">            <span class="keyword">if</span> self.Q_table[state][i]==Q_max:</span><br><span class="line">                a[i]=<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">ncol = <span class="number">12</span></span><br><span class="line">nrow = <span class="number">4</span></span><br><span class="line">env = CliffWalking(ncol, nrow)</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">alpha = <span class="number">0.1</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">agent = Sarsa(env, gamma, alpha,epsilon,<span class="number">4</span>)</span><br><span class="line">num_episodes = <span class="number">1000</span>  <span class="comment"># 智能体在环境中运行的序列的数量</span></span><br><span class="line"></span><br><span class="line">return_list = []  <span class="comment"># 记录每一条序列的回报</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># 显示10个进度条</span></span><br><span class="line">    <span class="comment"># tqdm的进度条功能</span></span><br><span class="line">    <span class="keyword">with</span> tqdm(total=<span class="built_in">int</span>(num_episodes / <span class="number">10</span>), desc=<span class="string">&#x27;Iteration %d&#x27;</span> % i) <span class="keyword">as</span> pbar:</span><br><span class="line">        <span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(num_episodes / <span class="number">10</span>)):  <span class="comment"># 每个进度条的序列数</span></span><br><span class="line">            episode_return = <span class="number">0</span></span><br><span class="line">            state = env.reset()</span><br><span class="line">            action = agent.take_action(state)</span><br><span class="line">            done = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">                next_state, reward, done = env.get_next_step(action)</span><br><span class="line">                next_action = agent.take_action(next_state)</span><br><span class="line">                episode_return += reward  <span class="comment"># 这里回报的计算不进行折扣因子衰减</span></span><br><span class="line">                agent.update_q_value(action, state, reward, next_action, next_state)</span><br><span class="line">                state = next_state</span><br><span class="line">                action = next_action</span><br><span class="line">            return_list.append(episode_return)</span><br><span class="line">            <span class="keyword">if</span> (i_episode + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:  <span class="comment"># 每10条序列打印一下这10条序列的平均回报</span></span><br><span class="line">                pbar.set_postfix(&#123;</span><br><span class="line">                    <span class="string">&#x27;episode&#x27;</span>:</span><br><span class="line">                    <span class="string">&#x27;%d&#x27;</span> % (num_episodes / <span class="number">10</span> * i + i_episode + <span class="number">1</span>),</span><br><span class="line">                    <span class="string">&#x27;return&#x27;</span>:</span><br><span class="line">                    <span class="string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="number">10</span>:])</span><br><span class="line">                &#125;)</span><br><span class="line">            pbar.update(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">episodes_list = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(return_list)))</span><br><span class="line">plt.plot(episodes_list, return_list)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Episodes&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Returns&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Sarsa on &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;Cliff Walking&#x27;</span>))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_agent</span>(<span class="params">agent, env, action_meaning, disaster=[], end=[]</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(env.row):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(env.col):</span><br><span class="line">            <span class="keyword">if</span> (i * env.col + j) <span class="keyword">in</span> disaster:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;****&#x27;</span>, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">            <span class="keyword">elif</span> (i * env.col + j) <span class="keyword">in</span> end:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;EEEE&#x27;</span>, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                a = agent.get_best_policy(i * env.col + j)</span><br><span class="line">                pi_str = <span class="string">&#x27;&#x27;</span></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(action_meaning)):</span><br><span class="line">                    pi_str += action_meaning[k] <span class="keyword">if</span> a[k] &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;o&#x27;</span></span><br><span class="line">                <span class="built_in">print</span>(pi_str, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">action_meaning = [<span class="string">&#x27;^&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;&lt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Sarsa算法最终收敛得到的策略为：&#x27;</span>)</span><br><span class="line">print_agent(agent, env, action_meaning, <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">37</span>, <span class="number">47</span>)), [<span class="number">47</span>])</span><br></pre></td></tr></table></figure>

<h3><span id="42-n-step-sarsa">4.2 n-step Sarsa</span></h3><p>Sarsa算法主要会在每一步都对Q表进行更新，但是这样的更新相比于在一串步骤之后再对Q表进行更新的TD算法而言，可能是存在一定偏差的，但TD算法一次更新所需要的episode太长，也不合适。<br>针对这些问题，可以选择使用n-step Sarsa方法，用episode中的n步进行更新。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NStepSarsa</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n, env, gamma, alpha, epsilon, n_action=<span class="number">4</span></span>):</span><br><span class="line">        self.n = n</span><br><span class="line">        self.env = env</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.n_action = n_action</span><br><span class="line">        self.Q_table = np.zeros([self.env.col * self.env.row, n_action])</span><br><span class="line">        self.states = []</span><br><span class="line">        self.actions = []</span><br><span class="line">        self.rewards = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">take_action</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="keyword">if</span> (np.random.random() &lt; self.epsilon):</span><br><span class="line">            action = np.random.randint(self.n_action)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.argmax(self.Q_table[state])</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_best_policy</span>(<span class="params">self, state</span>):</span><br><span class="line">        Q_max = np.<span class="built_in">max</span>(self.Q_table[state])</span><br><span class="line">        a = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_action)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_action):</span><br><span class="line">            <span class="keyword">if</span> self.Q_table[state][i] == Q_max:</span><br><span class="line">                a[i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_q_value</span>(<span class="params">self, at, st, r, atn, stn, done</span>):</span><br><span class="line">        self.states.append(st)</span><br><span class="line">        self.actions.append(at)</span><br><span class="line">        self.rewards.append(r)</span><br><span class="line">        <span class="keyword">if</span> self.n==<span class="built_in">len</span>(self.states):</span><br><span class="line">            G_n=self.Q_table[stn,atn]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(self.n)):</span><br><span class="line">                G_n=self.rewards[i]+self.gamma*G_n</span><br><span class="line">                <span class="keyword">if</span> done <span class="keyword">and</span> i&gt;<span class="number">0</span>:</span><br><span class="line">                    s=self.states[i]</span><br><span class="line">                    a=self.actions[i]</span><br><span class="line">                    self.Q_table[s,a]+=self.alpha*(G_n-self.Q_table[s,a])</span><br><span class="line">            s=self.states.pop(<span class="number">0</span>)</span><br><span class="line">            a=self.actions.pop(<span class="number">0</span>)</span><br><span class="line">            self.rewards.pop(<span class="number">0</span>)</span><br><span class="line">            self.Q_table[s,a]+=self.alpha*(G_n-self.Q_table[s,a])</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            self.states=[]</span><br><span class="line">            self.actions=[]</span><br><span class="line">            self.rewards=[]</span><br><span class="line"></span><br><span class="line"> <span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(num_episodes / <span class="number">10</span>)):  <span class="comment"># 每个进度条的序列数</span></span><br><span class="line">    episode_return = <span class="number">0</span></span><br><span class="line">    state = env.reset()</span><br><span class="line">    action = agent.take_action(state)</span><br><span class="line">    done = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        ext_state, reward, done = env.get_next_step(action)</span><br><span class="line">        next_action = agent.take_action(next_state)</span><br><span class="line">        episode_return += reward  <span class="comment"># 这里回报的计算不进行折扣因子衰减</span></span><br><span class="line">        agent.update_q_value(action, state, reward, next_action, next_state,done)</span><br><span class="line">        tate = next_state</span><br><span class="line">        action = next_action</span><br><span class="line">    return_list.append(episode_return)</span><br></pre></td></tr></table></figure>

<h3><span id="43-q-learning">4.3 Q-learning</span></h3><p>Q-learning主要是在Sarsa的基础上，在更新Q值的时候，采用了最优的action（即使用了最大的action value），而不是给出一个根据策略的action，并在此基础上进行筛选（相当于在Q-learning的过程中，TD target在选择a的时候进行了一定的优化。）<br>Q-learing中action的选择没有任何策略，但Sarsa则不然。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CliffWalking</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, col, row</span>):</span><br><span class="line">        self.col = col</span><br><span class="line">        self.row = row</span><br><span class="line">        self.x = <span class="number">0</span></span><br><span class="line">        self.y = row - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_next_step</span>(<span class="params">self, action</span>):</span><br><span class="line">        actions = [[<span class="number">0</span>, -<span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [-<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>]]</span><br><span class="line">        self.x = <span class="built_in">min</span>(<span class="built_in">max</span>(self.x + actions[action][<span class="number">0</span>], <span class="number">0</span>), self.col - <span class="number">1</span>)</span><br><span class="line">        self.y = <span class="built_in">min</span>(<span class="built_in">max</span>(self.y + actions[action][<span class="number">1</span>], <span class="number">0</span>), self.row - <span class="number">1</span>)</span><br><span class="line">        next_state = self.y * self.col + self.x</span><br><span class="line">        reward = -<span class="number">1</span></span><br><span class="line">        done = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> (self.y == self.row - <span class="number">1</span>) <span class="keyword">and</span> self.x &gt; <span class="number">0</span>:</span><br><span class="line">            done = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> self.x != self.col - <span class="number">1</span>:</span><br><span class="line">                reward = -<span class="number">100</span></span><br><span class="line">        <span class="keyword">return</span> next_state, reward, done</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.x = <span class="number">0</span></span><br><span class="line">        self.y = self.row - <span class="number">1</span></span><br><span class="line">        next_state = self.y * self.col + self.x</span><br><span class="line">        <span class="keyword">return</span> next_state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QLearning</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n, env, gamma, alpha, epsilon, n_action=<span class="number">4</span></span>):</span><br><span class="line">        self.n = n</span><br><span class="line">        self.env = env</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.n_action = n_action</span><br><span class="line">        self.Q_table = np.zeros([self.env.col * self.env.row, n_action])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">take_action</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="keyword">if</span> (np.random.random() &lt; self.epsilon):</span><br><span class="line">            action = np.random.randint(self.n_action)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.argmax(self.Q_table[state])</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_best_policy</span>(<span class="params">self, state</span>):</span><br><span class="line">        Q_max = np.<span class="built_in">max</span>(self.Q_table[state])</span><br><span class="line">        a = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_action)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_action):</span><br><span class="line">            <span class="keyword">if</span> self.Q_table[state][i] == Q_max:</span><br><span class="line">                a[i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_q_value</span>(<span class="params">self, at, st, r, st1</span>):</span><br><span class="line">        TD_error=r+self.gamma*self.Q_table[st1].<span class="built_in">max</span>()-self.Q_table[st,at]</span><br><span class="line">        self.Q_table[st,at]+=self.alpha*TD_error</span><br><span class="line"></span><br><span class="line">ncol=<span class="number">12</span></span><br><span class="line">nrow=<span class="number">4</span></span><br><span class="line">env=CliffWalking(ncol,nrow)</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">n_step = <span class="number">5</span>  <span class="comment"># 5步Sarsa算法</span></span><br><span class="line">alpha = <span class="number">0.1</span></span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">agent = QLearning(n_step, env,gamma,alpha,epsilon)</span><br><span class="line">num_episodes = <span class="number">500</span>  <span class="comment"># 智能体在环境中运行的序列的数量</span></span><br><span class="line"></span><br><span class="line">return_list = []  <span class="comment"># 记录每一条序列的回报</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># 显示10个进度条</span></span><br><span class="line">    <span class="comment">#tqdm的进度条功能</span></span><br><span class="line">    <span class="keyword">with</span> tqdm(total=<span class="built_in">int</span>(num_episodes / <span class="number">10</span>), desc=<span class="string">&#x27;Iteration %d&#x27;</span> % i) <span class="keyword">as</span> pbar:</span><br><span class="line">        <span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(num_episodes / <span class="number">10</span>)):  <span class="comment"># 每个进度条的序列数</span></span><br><span class="line">            episode_return = <span class="number">0</span></span><br><span class="line">            state = env.reset()</span><br><span class="line">            done = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">                action = agent.take_action(state)</span><br><span class="line">                next_state, reward, done = env.get_next_step(action)</span><br><span class="line">                episode_return += reward  <span class="comment"># 这里回报的计算不进行折扣因子衰减</span></span><br><span class="line">                agent.update_q_value(action, state, reward,next_state)</span><br><span class="line">                state = next_state</span><br><span class="line">            return_list.append(episode_return)</span><br><span class="line">            <span class="keyword">if</span> (i_episode + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:  <span class="comment"># 每10条序列打印一下这10条序列的平均回报</span></span><br><span class="line">                pbar.set_postfix(&#123;</span><br><span class="line">                    <span class="string">&#x27;episode&#x27;</span>:</span><br><span class="line">                    <span class="string">&#x27;%d&#x27;</span> % (num_episodes / <span class="number">10</span> * i + i_episode + <span class="number">1</span>),</span><br><span class="line">                    <span class="string">&#x27;return&#x27;</span>:</span><br><span class="line">                    <span class="string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="number">10</span>:])</span><br><span class="line">                &#125;)</span><br><span class="line">            pbar.update(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">episodes_list = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(return_list)))</span><br><span class="line">plt.plot(episodes_list, return_list)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Episodes&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Returns&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Q-learning on &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;Cliff Walking&#x27;</span>))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_agent</span>(<span class="params">agent, env, action_meaning, disaster=[], end=[]</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(env.row):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(env.col):</span><br><span class="line">            <span class="keyword">if</span> (i * env.col + j) <span class="keyword">in</span> disaster:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;****&#x27;</span>, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">            <span class="keyword">elif</span> (i * env.col + j) <span class="keyword">in</span> end:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;EEEE&#x27;</span>, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                a = agent.get_best_policy(i * env.col + j)</span><br><span class="line">                pi_str = <span class="string">&#x27;&#x27;</span></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(action_meaning)):</span><br><span class="line">                    pi_str += action_meaning[k] <span class="keyword">if</span> a[k] &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;o&#x27;</span></span><br><span class="line">                <span class="built_in">print</span>(pi_str, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">action_meaning = [<span class="string">&#x27;^&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;&lt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Q-learning算法最终收敛得到的策略为：&#x27;</span>)</span><br><span class="line">print_agent(agent, env, action_meaning, <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">37</span>, <span class="number">47</span>)), [<span class="number">47</span>])</span><br></pre></td></tr></table></figure>

<h2><span id="关于一些区分">* 关于一些区分</span></h2><h3><span id="1-on-policy和off-policy">*.1 on-policy和off-policy</span></h3><p>与环境交互的policy被称作behavior policy，用于生成episode，而达成目标的policy被称为target policy；二者相同则为on-policy，不同则是off-policy<br>其中，Sarsa是on-policy，Q-learning是off-policy</p>
<h2><span id="5-dyna-q">5 Dyna-Q</span></h2><p>这是一种有模型的强化学习方法，通过真实数据去构建模型，并用一种叫做Q-Planning的方法进行强化学习。<br>这里使用了字典作为模型，事实上可以使用状态方程之类的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CliffWalking</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, col, row</span>):</span><br><span class="line">        self.col = col</span><br><span class="line">        self.row = row</span><br><span class="line">        self.x = <span class="number">0</span></span><br><span class="line">        self.y = row - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_next_step</span>(<span class="params">self, action</span>):</span><br><span class="line">        actions = [[<span class="number">0</span>, -<span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [-<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>]]</span><br><span class="line">        self.x = <span class="built_in">min</span>(<span class="built_in">max</span>(self.x + actions[action][<span class="number">0</span>], <span class="number">0</span>), self.col - <span class="number">1</span>)</span><br><span class="line">        self.y = <span class="built_in">min</span>(<span class="built_in">max</span>(self.y + actions[action][<span class="number">1</span>], <span class="number">0</span>), self.row - <span class="number">1</span>)</span><br><span class="line">        next_state = self.y * self.col + self.x</span><br><span class="line">        reward = -<span class="number">1</span></span><br><span class="line">        done = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> (self.y == self.row - <span class="number">1</span>) <span class="keyword">and</span> self.x &gt; <span class="number">0</span>:</span><br><span class="line">            done = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> self.x != self.col - <span class="number">1</span>:</span><br><span class="line">                reward = -<span class="number">100</span></span><br><span class="line">        <span class="keyword">return</span> next_state, reward, done</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.x = <span class="number">0</span></span><br><span class="line">        self.y = self.row - <span class="number">1</span></span><br><span class="line">        next_state = self.y * self.col + self.x</span><br><span class="line">        <span class="keyword">return</span> next_state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DynaQ</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, env, gamma, alpha, epsilon, n_planning,n_action=<span class="number">4</span></span>):</span><br><span class="line">        self.env = env</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.n_action = n_action</span><br><span class="line">        self.Q_table = np.zeros([self.env.col * self.env.row, n_action])</span><br><span class="line"></span><br><span class="line">        self.n_planning=n_planning</span><br><span class="line">        self.model=<span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">take_action</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="keyword">if</span> (np.random.random() &lt; self.epsilon):</span><br><span class="line">            action = np.random.randint(self.n_action)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.argmax(self.Q_table[state])</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_best_policy</span>(<span class="params">self, state</span>):</span><br><span class="line">        Q_max = np.<span class="built_in">max</span>(self.Q_table[state])</span><br><span class="line">        a = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_action)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_action):</span><br><span class="line">            <span class="keyword">if</span> self.Q_table[state][i] == Q_max:</span><br><span class="line">                a[i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_q_value</span>(<span class="params">self, at, st, r, st1</span>):</span><br><span class="line">        TD_error=r+self.gamma*self.Q_table[st1].<span class="built_in">max</span>()-self.Q_table[st,at]</span><br><span class="line">        self.Q_table[st,at]+=self.alpha*TD_error</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self,at,st,r,st1</span>):</span><br><span class="line">        self.update_q_value(at,st,r,st1)</span><br><span class="line">        self.model[(st,at)]=r,st1</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_planning):</span><br><span class="line">            (sm,am),(rm,s_m)=random.choice(<span class="built_in">list</span>(self.model.items()))</span><br><span class="line">            self.update_q_value(am,sm,rm,s_m)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DynaQ_CliffWalking</span>(<span class="params">n_planning</span>):</span><br><span class="line">    ncol = <span class="number">12</span></span><br><span class="line">    nrow = <span class="number">4</span></span><br><span class="line">    env = CliffWalking(ncol, nrow)</span><br><span class="line">    epsilon = <span class="number">0.01</span></span><br><span class="line">    alpha = <span class="number">0.1</span></span><br><span class="line">    gamma = <span class="number">0.9</span></span><br><span class="line">    agent = DynaQ(env, gamma, alpha, epsilon, n_planning)</span><br><span class="line">    num_episodes = <span class="number">300</span>  <span class="comment"># 智能体在环境中运行多少条序列</span></span><br><span class="line"></span><br><span class="line">    return_list = []  <span class="comment"># 记录每一条序列的回报</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># 显示10个进度条</span></span><br><span class="line">        <span class="comment"># tqdm的进度条功能</span></span><br><span class="line">        <span class="keyword">with</span> tqdm(total=<span class="built_in">int</span>(num_episodes / <span class="number">10</span>),</span><br><span class="line">                  desc=<span class="string">&#x27;Iteration %d&#x27;</span> % i) <span class="keyword">as</span> pbar:</span><br><span class="line">            <span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(num_episodes / <span class="number">10</span>)):  <span class="comment"># 每个进度条的序列数</span></span><br><span class="line">                episode_return = <span class="number">0</span></span><br><span class="line">                state = env.reset()</span><br><span class="line">                done = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">                    action = agent.take_action(state)</span><br><span class="line">                    next_state, reward, done = env.get_next_step(action)</span><br><span class="line">                    episode_return += reward  <span class="comment"># 这里回报的计算不进行折扣因子衰减</span></span><br><span class="line">                    agent.update(action, state, reward, next_state)</span><br><span class="line">                    state = next_state</span><br><span class="line">                return_list.append(episode_return)</span><br><span class="line">                <span class="keyword">if</span> (i_episode + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:  <span class="comment"># 每10条序列打印一下这10条序列的平均回报</span></span><br><span class="line">                    pbar.set_postfix(&#123;</span><br><span class="line">                        <span class="string">&#x27;episode&#x27;</span>:</span><br><span class="line">                        <span class="string">&#x27;%d&#x27;</span> % (num_episodes / <span class="number">10</span> * i + i_episode + <span class="number">1</span>),</span><br><span class="line">                        <span class="string">&#x27;return&#x27;</span>:</span><br><span class="line">                        <span class="string">&#x27;%.3f&#x27;</span> % np.mean(return_list[-<span class="number">10</span>:])</span><br><span class="line">                    &#125;)</span><br><span class="line">                pbar.update(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> return_list</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">random.seed(<span class="number">0</span>)</span><br><span class="line">n_planning_list = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">20</span>]</span><br><span class="line"><span class="keyword">for</span> n_planning <span class="keyword">in</span> n_planning_list:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Q-planning步数为：%d&#x27;</span> % n_planning)</span><br><span class="line">    time.sleep(<span class="number">0.5</span>)</span><br><span class="line">    return_list = DynaQ_CliffWalking(n_planning)</span><br><span class="line">    episodes_list = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(return_list)))</span><br><span class="line">    plt.plot(episodes_list,</span><br><span class="line">             return_list,</span><br><span class="line">             label=<span class="built_in">str</span>(n_planning) + <span class="string">&#x27; planning steps&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Episodes&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Returns&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Dyna-Q on &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;Cliff Walking&#x27;</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RL/" rel="tag">RL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li></ul>

    </footer>
  </div>

   
   
  
</article>

    
  </article>
  

  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2024
        <i class="ri-heart-fill heart_icon"></i> suixinhita
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/Logo_rhodesOverride.png" alt="RHODES ISLAND"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js"></script>
<script src="https://cdn.staticfile.org/mathjax/2.7.7/config/TeX-AMS-MML_HTMLorMML-full.js"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->

<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>